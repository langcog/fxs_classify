\relax 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\citation{Hagerman:2008wg}
\citation{Cohen:1988vx}
\citation{Cohen:1989cm}
\citation{Kennedy:2001dg}
\citation{Rehg}
\@writefile{toc}{\contentsline {section}{\numberline {1}\hskip -1em.\nobreakspace  {}Introduction}{1}{section.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces We perform gaze-tracking and facial-analysis to study participants (1) with mental impairments engaging in social interactions with an interviewer (2), that behave differently depending on their mental disorder. The goal of our work is to classify between disorders using this data.}}{1}{figure.1}}
\newlabel{fig:pull_figure}{{1}{1}{We perform gaze-tracking and facial-analysis to study participants (1) with mental impairments engaging in social interactions with an interviewer (2), that behave differently depending on their mental disorder. The goal of our work is to classify between disorders using this data}{figure.1}{}}
\@writefile{brf}{\backcite{Hagerman:2008wg}{{1}{1}{figure.1}}}
\@writefile{brf}{\backcite{Cohen:1988vx,Cohen:1989cm}{{1}{1}{figure.1}}}
\@writefile{brf}{\backcite{Kennedy:2001dg}{{1}{1}{figure.1}}}
\@writefile{brf}{\backcite{Rehg}{{1}{1}{figure.1}}}
\citation{Kumar}
\citation{sabeti}
\citation{Boraston}
\citation{hashemi}
\citation{dalton}
\citation{Hall:2006jd}
\citation{Pimentel:1999vx}
\citation{Sullivan:2007gz}
\citation{Sullivan:2006jc}
\citation{Hall:2012ge}
\citation{Csibra:2006wf}
\citation{Morales:2000br}
\citation{Emery:2000ug}
\citation{DohertySneddon:2013fv}
\citation{Riby:2012il}
\citation{Farzin:2009fq}
\citation{Farzin:2011ip}
\citation{AmodelofsaliencybasedvisualattentionforrapidsceneanalysisItti:tq}
\citation{HANDEYE}
\citation{Borji:2012dq}
\citation{Fathi:2012vk}
\citation{Fathi:2013tc}
\citation{FromEgotoNosVisionDetectingSocialRelationshipsinFirstPersonViewsAlletto:wi}
\citation{linda}
\@writefile{toc}{\contentsline {section}{\numberline {2}\hskip -1em.\nobreakspace  {}Previous Work}{2}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}\hskip -1em.\nobreakspace  {}Classification of disorders}{2}{subsection.2.1}}
\@writefile{brf}{\backcite{Kumar, sabeti}{{2}{2.1}{subsection.2.1}}}
\@writefile{brf}{\backcite{Boraston, hashemi, dalton}{{2}{2.1}{subsection.2.1}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}\hskip -1em.\nobreakspace  {}Cognitive Impairment Studies}{2}{subsection.2.2}}
\@writefile{brf}{\backcite{Hall:2006jd,Pimentel:1999vx,Sullivan:2007gz,Sullivan:2006jc}{{2}{2.2}{subsection.2.2}}}
\@writefile{brf}{\backcite{Hall:2012ge}{{2}{2.2}{subsection.2.2}}}
\@writefile{brf}{\backcite{Csibra:2006wf, Morales:2000br,Emery:2000ug}{{2}{2.2}{subsection.2.2}}}
\@writefile{brf}{\backcite{DohertySneddon:2013fv,Riby:2012il}{{2}{2.2}{subsection.2.2}}}
\@writefile{brf}{\backcite{Farzin:2009fq, Farzin:2011ip}{{2}{2.2}{subsection.2.2}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}\hskip -1em.\nobreakspace  {}The mechanism for human visual attention}{2}{subsection.2.3}}
\@writefile{brf}{\backcite{AmodelofsaliencybasedvisualattentionforrapidsceneanalysisItti:tq}{{2}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{HANDEYE, Borji:2012dq}{{2}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{Fathi:2012vk}{{2}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{Fathi:2013tc}{{2}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{FromEgotoNosVisionDetectingSocialRelationshipsinFirstPersonViewsAlletto:wi}{{2}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{linda}{{2}{2.3}{subsection.2.3}}}
\@writefile{toc}{\contentsline {section}{\numberline {3}\hskip -1em.\nobreakspace  {}Dataset}{2}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}\hskip -1em.\nobreakspace  {}Participants}{2}{subsection.3.1}}
\citation{dpmface}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}\hskip -1em.\nobreakspace  {}Environmental Set-Up}{3}{subsection.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A frame from videos showing the participant's view (participant's head is visible in the bottom of the frame). Eye-movements are tracked with a remote eye-tracker and mapped into the coordinate space of this video.}}{3}{figure.2}}
\newlabel{fig:environment}{{2}{3}{A frame from videos showing the participant's view (participant's head is visible in the bottom of the frame). Eye-movements are tracked with a remote eye-tracker and mapped into the coordinate space of this video}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}\hskip -1em.\nobreakspace  {}System Architecture}{3}{section.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Face Motion Analysis. (a) The worst case of high motion of the interviewer; (b) the most steady interviewer. In all cases the interviewers are instructed to limit their head movement as much as possible; (c) and (d) represent the 20 px threshold radius surrounding the face key points. This threshold results in full face coverage across all interviews.}}{3}{figure.3}}
\newlabel{fig:PERSON}{{3}{3}{Face Motion Analysis. (a) The worst case of high motion of the interviewer; (b) the most steady interviewer. In all cases the interviewers are instructed to limit their head movement as much as possible; (c) and (d) represent the 20 px threshold radius surrounding the face key points. This threshold results in full face coverage across all interviews}{figure.3}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{3}{figure.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{3}{figure.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{3}{figure.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {}}}{3}{figure.3}}
\@writefile{toc}{\contentsline {section}{\numberline {5}\hskip -1em.\nobreakspace  {}Feature Extraction}{3}{section.5}}
\newlabel{sec:feature_extraction}{{5}{3}{\hskip -1em.~Feature Extraction\relax }{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}\hskip -1em.\nobreakspace  {}Face-mark Extraction}{3}{subsection.5.1}}
\@writefile{brf}{\backcite{dpmface}{{3}{5.1}{subsection.5.1}}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces System Architecture. The environment is composed of an interviewer (1) facing a participant (2) who wears a remote eye-tracker (3) while a fixed-camera (4) faces the interviewer. Face marks (5) on the interviewer's face are extracted from the raw video and we map the eye-tracking data onto these marks. This mapping is then clustered into semantic facial regions (6) and we represent these facial regions using state vectors (7). Sub-sequences of features (8) are used to train classification algorithms (9). Finally, the full feature sequences of unseen participants (10) are used to predict their mental disorder (11).}}{4}{figure.4}}
\newlabel{fig:system_architecture}{{4}{4}{System Architecture. The environment is composed of an interviewer (1) facing a participant (2) who wears a remote eye-tracker (3) while a fixed-camera (4) faces the interviewer. Face marks (5) on the interviewer's face are extracted from the raw video and we map the eye-tracking data onto these marks. This mapping is then clustered into semantic facial regions (6) and we represent these facial regions using state vectors (7). Sub-sequences of features (8) are used to train classification algorithms (9). Finally, the full feature sequences of unseen participants (10) are used to predict their mental disorder (11)}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}\hskip -1em.\nobreakspace  {}Feature Construction}{4}{subsection.5.2}}
\newlabel{ssec:feature_construction}{{5.2}{4}{\hskip -1em.~Feature Construction\relax }{subsection.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Clustering of the 69 key-marks of the interviewer's face into 5 semantic facial regions. The colors represent the clustering of these marks into 5 facial regions. A 6th region, not shown, is reserved for when the participants look away from the face. These clusters represents our features.}}{4}{figure.5}}
\newlabel{fig:feats}{{5}{4}{Clustering of the 69 key-marks of the interviewer's face into 5 semantic facial regions. The colors represent the clustering of these marks into 5 facial regions. A 6th region, not shown, is reserved for when the participants look away from the face. These clusters represents our features}{figure.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}\hskip -1em.\nobreakspace  {}Notation}{4}{subsection.5.3}}
\newlabel{sec:notation}{{5.3}{4}{\hskip -1em.~Notation\relax }{subsection.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}\hskip -1em.\nobreakspace  {}Fourier Analysis of Features}{4}{subsection.5.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Facial Region State Vectors. The 5 facial regions and the non-face region are represented as state-vector features $f_i$. A sliding window technique is used to extract the input into our classifiers.}}{4}{figure.6}}
\newlabel{fig:statevectors}{{6}{4}{Facial Region State Vectors. The 5 facial regions and the non-face region are represented as state-vector features $f_i$. A sliding window technique is used to extract the input into our classifiers}{figure.6}{}}
\citation{Restrepo:2014gs}
\citation{entrophy}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Feature histograms for the various disorders. X-axis represents different states: non-face (0), nose (1), eye-left (2), eye-right (3), mouth (4), and jaw (5). (a)-(c) feature histograms for all participants of each class. (d)-(f) equivalent histograms with the non-face state vector removed.}}{5}{figure.7}}
\newlabel{fig:histo}{{7}{5}{Feature histograms for the various disorders. X-axis represents different states: non-face (0), nose (1), eye-left (2), eye-right (3), mouth (4), and jaw (5). (a)-(c) feature histograms for all participants of each class. (d)-(f) equivalent histograms with the non-face state vector removed}{figure.7}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces { DD}}}{5}{figure.7}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {FXS-F}}}{5}{figure.7}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {FXS-M}}}{5}{figure.7}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {DD}}}{5}{figure.7}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {FXS-F}}}{5}{figure.7}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {FXS-M}}}{5}{figure.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}\hskip -1em.\nobreakspace  {}Feature Granularity}{5}{subsection.5.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6}\hskip -1em.\nobreakspace  {}Attentional transitions}{5}{subsection.5.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7}\hskip -1em.\nobreakspace  {}Approximate Entropy}{5}{subsection.5.7}}
\@writefile{brf}{\backcite{Restrepo:2014gs,entrophy}{{5}{5.7}{subsection.5.7}}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Matrix of attentional transitions for each disorder. Each square $[ij]$ represents the aggregated number of times participants of each group transitioned attention from state $i$ to state $j$. The axes represent the different states: non-face (0), nose (1), eye-left (2), eye-right (3), mouth (4), and jaw (5).}}{5}{figure.8}}
\newlabel{fig:transitions}{{8}{5}{Matrix of attentional transitions for each disorder. Each square $[ij]$ represents the aggregated number of times participants of each group transitioned attention from state $i$ to state $j$. The axes represent the different states: non-face (0), nose (1), eye-left (2), eye-right (3), mouth (4), and jaw (5)}{figure.8}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {DD}}}{5}{figure.8}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {FXS-F}}}{5}{figure.8}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {FXS-M}}}{5}{figure.8}}
\citation{Krizhevsky:2012wl}
\citation{hamidss}
\citation{Bouvrie:2006vb}
\citation{lecun}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Temporal analysis of attention to face. X axis represents time in frames (in increments of 0.2 seconds). Y axis represents each participant. Black dot represent time points when the participant was looking at the interviewer's face. White space signifies that they were not.}}{6}{figure.9}}
\newlabel{fig:sticky}{{9}{6}{Temporal analysis of attention to face. X axis represents time in frames (in increments of 0.2 seconds). Y axis represents each participant. Black dot represent time points when the participant was looking at the interviewer's face. White space signifies that they were not}{figure.9}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {DD - Group}}}{6}{figure.9}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {FXS Female}}}{6}{figure.9}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {FXS Male}}}{6}{figure.9}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Analysis of the average $ApEN$ of the data for each participant class. X-axis represents (a) the tolerance parameter $r = R/std(Q)$, (b) the dimension parameter $w$.}}{6}{figure.10}}
\newlabel{fig:entropy}{{10}{6}{Analysis of the average $ApEN$ of the data for each participant class. X-axis represents (a) the tolerance parameter $r = R/std(Q)$, (b) the dimension parameter $w$}{figure.10}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Tolerance Parameter}}}{6}{figure.10}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Dimension}}}{6}{figure.10}}
\@writefile{toc}{\contentsline {section}{\numberline {6}\hskip -1em.\nobreakspace  {}Training of Classifiers}{6}{section.6}}
\newlabel{sec:classification}{{6}{6}{\hskip -1em.~Training of Classifiers\relax }{section.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}\hskip -1em.\nobreakspace  {}State Vector CNN}{6}{subsection.6.1}}
\newlabel{sec:CNN}{{6.1}{6}{\hskip -1em.~State Vector CNN\relax }{subsection.6.1}{}}
\@writefile{brf}{\backcite{Krizhevsky:2012wl, hamidss, Bouvrie:2006vb,lecun}{{6}{6.1}{subsection.6.1}}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Analysis of the $ApEn$ of the data per individual varying the dimension parameter $w$. Y-axis is $ApEn$ and X-axis varies $w$. Each line represents one participant's data.}}{6}{figure.11}}
\newlabel{fig:individual_entropy}{{11}{6}{Analysis of the $ApEn$ of the data per individual varying the dimension parameter $w$. Y-axis is $ApEn$ and X-axis varies $w$. Each line represents one participant's data}{figure.11}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {DD}}}{6}{figure.11}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {FXS-F}}}{6}{figure.11}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {FXS-M}}}{6}{figure.11}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Convolutional Neural Network Design. The input is a binary vector representing a sequence of features. The CNN is composed of a single hidden layer and an output layer. The filters are chosen to be an integer multiple of 6, the length of the state vectors. The output is a sigmoid, thresholded to 0.5 for classification}}{7}{figure.12}}
\newlabel{fig:conv}{{12}{7}{Convolutional Neural Network Design. The input is a binary vector representing a sequence of features. The CNN is composed of a single hidden layer and an output layer. The filters are chosen to be an integer multiple of 6, the length of the state vectors. The output is a sigmoid, thresholded to 0.5 for classification\relax }{figure.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}\hskip -1em.\nobreakspace  {}Other Classifiers}{7}{subsection.6.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Analysis of SVM classifier training and testing error as a function of time window (in seconds) for pair-wise classifiers. One second of time corresponds to 5 features.}}{7}{figure.13}}
\newlabel{fig:window_lengths}{{13}{7}{Analysis of SVM classifier training and testing error as a function of time window (in seconds) for pair-wise classifiers. One second of time corresponds to 5 features}{figure.13}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {DD vs FXS Female}}}{7}{figure.13}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {DD vs FXS Male}}}{7}{figure.13}}
\bibstyle{ieee}
\bibdata{fxs-cvpr}
\bibcite{AmodelofsaliencybasedvisualattentionforrapidsceneanalysisItti:tq}{1}
\@writefile{toc}{\contentsline {section}{\numberline {7}\hskip -1em.\nobreakspace  {}Participant Classification}{8}{section.7}}
\newlabel{sec:participant_classification}{{7}{8}{\hskip -1em.~Participant Classification\relax }{section.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}\hskip -1em.\nobreakspace  {}Experiments and Results}{8}{subsection.7.1}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Comparison of precision of our system against other classifiers. Columns denote pairwise classification precision of participants for DD vs FXS-female and DD vs FXS-male binary classification. Classifiers are run on 3,10, and 50 second time windows. We compare our system classifier, S-CNN, to SVM, NB, and HMM algorithms.}}{8}{table.1}}
\newlabel{table:profiler}{{1}{8}{Comparison of precision of our system against other classifiers. Columns denote pairwise classification precision of participants for DD vs FXS-female and DD vs FXS-male binary classification. Classifiers are run on 3,10, and 50 second time windows. We compare our system classifier, S-CNN, to SVM, NB, and HMM algorithms}{table.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}\hskip -1em.\nobreakspace  {}Discussion}{8}{section.8}}
\bibcite{hamidss}{2}
\bibcite{entrophy}{3}
\bibcite{Boraston}{4}
\bibcite{Borji:2012dq}{5}
\bibcite{Bouvrie:2006vb}{6}
\bibcite{Cohen:1988vx}{7}
\bibcite{Cohen:1989cm}{8}
\bibcite{Csibra:2006wf}{9}
\bibcite{dalton}{10}
\bibcite{DohertySneddon:2013fv}{11}
\bibcite{Emery:2000ug}{12}
\bibcite{Farzin:2009fq}{13}
\bibcite{Farzin:2011ip}{14}
\bibcite{Fathi:2012vk}{15}
\bibcite{Fathi:2013tc}{16}
\bibcite{FromEgotoNosVisionDetectingSocialRelationshipsinFirstPersonViewsAlletto:wi}{17}
\bibcite{Hagerman:2008wg}{18}
\bibcite{Hall:2006jd}{19}
\bibcite{Hall:2012ge}{20}
\bibcite{hashemi}{21}
\bibcite{Kennedy:2001dg}{22}
\bibcite{Krizhevsky:2012wl}{23}
\bibcite{Kumar}{24}
\bibcite{lecun}{25}
\bibcite{Morales:2000br}{26}
\bibcite{Pimentel:1999vx}{27}
\bibcite{Rehg}{28}
\bibcite{Restrepo:2014gs}{29}
\bibcite{Riby:2012il}{30}
\bibcite{sabeti}{31}
\bibcite{Sullivan:2006jc}{32}
\bibcite{Sullivan:2007gz}{33}
\bibcite{HANDEYE}{34}
\bibcite{linda}{35}
\bibcite{dpmface}{36}
